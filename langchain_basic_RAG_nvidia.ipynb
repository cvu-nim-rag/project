{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe",
      "metadata": {
        "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe"
      },
      "source": [
        "# Part 1 pre-acceptance workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27",
      "metadata": {
        "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27"
      },
      "source": [
        "This notebook is meant to be a prototype for the NVIDIA NIM hackathon project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f",
      "metadata": {
        "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f"
      },
      "source": [
        "## Installation and Requirements\n",
        "\n",
        "Shits fucked cunt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
        "outputId": "2f26a4cb-ef97-4590-c9c8-c1118e1f3b65"
      },
      "outputs": [],
      "source": [
        "# Requirements\n",
        "!pip install langchain==0.2.5\n",
        "!pip install langchain_community==0.2.5\n",
        "!pip install faiss-gpu # replace with faiss-gpu if you are using GPU\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain-nvidia-ai-endpoints==0.1.2\n",
        "!pip install requests pdfplumber spacy camelot-py \n",
        "!pip install pandas==2 numpy==1.26.4 \n",
        "!pip install beautifulsoup4 \n",
        "!pip install pymupdf\n",
        "!pip install lxml\n",
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84",
      "metadata": {
        "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84"
      },
      "source": [
        "## Getting Started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04495732-c2db-4c97-91d0-96708814334d",
      "metadata": {
        "id": "04495732-c2db-4c97-91d0-96708814334d"
      },
      "source": [
        "You need an `NVIDIA_API_KEY` to use the NVIDIA API Catalog:\n",
        "\n",
        "1) Create a free account with [NVIDIA](https://build.nvidia.com/explore/discover).\n",
        "2) Click on your model of choice.\n",
        "3) Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
        "4) Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints.\n",
        "5) If at any point downstream NVIDIA complains about insufficient credits, tell them to shut the fuck up and make a new gmail. If their stock price is so high, they should share some of those profits with me, an NVIDIA investor (i own 3 shares rn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7fd4493",
      "metadata": {
        "id": "c7fd4493"
      },
      "source": [
        "Available NVIDIA api keys;\n",
        "\n",
        "1. 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\n",
        "2. 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
        "outputId": "e4d2f493-998c-42ab-8d7e-60c7cffcd318"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
        "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
        "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6",
      "metadata": {
        "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6"
      },
      "source": [
        "## LLM & Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886",
      "metadata": {
        "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886"
      },
      "source": [
        "### 1) Initialize the LLM\n",
        "\n",
        "The ChatNVIDIA class is part of LangChain's integration (langchain_nvidia_ai_endpoints) with NVIDIA NIM microservices.\n",
        "It allows access to NVIDIA NIM for chat applications, connecting to hosted or locally-deployed microservices.\n",
        "\n",
        "Here we will use **mixtral-8x7b-instruct-v0.1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
        "outputId": "fa6a7c4a-2ddc-4d73-f511-859bce8769f2"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", max_tokens=1024)\n",
        "\n",
        "# Can choose any model hosted at Nvidia API Catalog (Uncomment the below code to list the availabe models)\n",
        "# ChatNVIDIA.get_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19acd7b4",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- In this notebook, we have used NVIDIA NIM microservices from the NVIDIA API Catalog.\n",
        "- The other APIs, ChatNVIDIA, NVIDIAEmbedding, and NVIDIARerank, also support self-hosted NIM microservices.\n",
        "- Change the `base_url` to your deployed NIM URL.\n",
        "- Example: `llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")`\n",
        "- NIM can be also hosted locally using Docker, following the [NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) documentation. This is only true if you are the son of a rich oil tycoon, and have a few H100s sitting around in your basement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f2dcf372",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Code snippet if you want to use a self-hosted NIM\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "# connect to an LLM NIM running at localhost:8000, specifying a specific model\n",
        "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6",
      "metadata": {
        "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6"
      },
      "source": [
        "### 2) Intialize the embedding\n",
        "NVIDIAEmbeddings is a client to NVIDIA embeddings models that provides access to a NVIDIA NIM for embedding. It can connect to a hosted NIM or a local NIM using a base URL\n",
        "\n",
        "We selected **NV-Embed-QA** as the embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017",
      "metadata": {
        "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9862f2e-5055-4fe4-818d-708091243d74",
      "metadata": {
        "id": "b9862f2e-5055-4fe4-818d-708091243d74"
      },
      "source": [
        "### 3) Obtain dataset\n",
        "I love taxes and work! Lets steal information about taxes and work!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64106ff",
      "metadata": {
        "id": "b64106ff"
      },
      "source": [
        "#### In the **DataHandler** class defined below, we can;\n",
        "\n",
        "A) Walk through a webpage and find all sub-webpages and scrape the parent and children,\n",
        "\n",
        "B) Extract texts, tables and images from pdfs\n",
        "\n",
        "Real world documents can be very long, this makes it hard to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
        "\n",
        "To handle this weâ€™ll split the Document into chunks for embedding and vector storage. More on text splitting [here](https://python.langchain.com/v0.2/docs/concepts/#text-splitters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "8c07d429",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8c07d429",
        "outputId": "86e892d9-a219-4dd6-8a18-71daf5ee84e9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import urllib.parse  # To handle URL joining\n",
        "import fitz\n",
        "\n",
        "from tqdm import tqdm\n",
        "from io import StringIO\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, DataFrameLoader, CSVLoader, UnstructuredTSVLoader\n",
        "\n",
        "class DataHandler:\n",
        "    \"\"\"\n",
        "    Masterfully handles data scraping, preprocessing, and other data-related functionalities in this notebook.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                csv_path=\"./data/csv\"):\n",
        "        self.visited_urls = set() # classwide tracker to prevent repeated visits\n",
        "        self.webloaders = [] # tracks all urls that have been converted to langchain WebBaseLoaders.\n",
        "        self.tabular = [] # tracks all tabular data that has been discovered by scraper. Delivers a list of CSVLoader. Cowardly refusing to save to same list\n",
        "        self.raws = []\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "        self.text_splitter = CharacterTextSplitter(chunk_size=400, separator=\" \", chunk_overlap=80)\n",
        "\n",
        "        self.visited_urls = set()   # Classwide tracker to prevent repeated visits when scraping web\n",
        "        self.tabular_data = []      # Tracks all tabular_data data that has been discovered by scraper. Delivers a list of CSVLoader. Cowardly refusing to save to same list.\n",
        "        self.textual_data = []      # Tracks all textual data scraped from websites or pdfs\n",
        "        self.all_data = []  # Just defining a joint list here instead of later during the functional call; in case we do not care about seperating them anymore (both are lists of Documents, just from different\n",
        "        # base sources)\n",
        "\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            os.mkdir(self.csv_path)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_csv(csv_path):\n",
        "        \"\"\"\n",
        "        Small func to read from csv and produce CSVLoaders.\n",
        "        \"\"\"\n",
        "        df = pd.from_csv( csv_path) # so we can yoink its columns\n",
        "        loader = CSVLoader(file_path=csv_path,\n",
        "                                    csv_args={  'delimiter': ',',\n",
        "                                                'quotechar': '\"',\n",
        "                                                'fieldnames': [str(col) for col in df.columns]}\n",
        "                                    )\n",
        "        return loader\n",
        "\n",
        "    def csvs_to_loader(self, directory):\n",
        "        \"\"\"\n",
        "        walks through directory, finds all csvs, and saves it into self.tabular \n",
        "        \"\"\"\n",
        "        for dir, subdir, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.endswith(\".csv\"):\n",
        "                    fp = os.path.join(dir, file)\n",
        "                    loader = self.from_csv(fp)\n",
        "                    self.tabular.append(loader)\n",
        "\n",
        "\n",
        "    def extract_table_elements(self, url, table_elements):\n",
        "        \"\"\"\n",
        "        Helper func to extract tabular data\n",
        "\n",
        "        Args:\n",
        "            - url: url the table is under\n",
        "            - table_elements: list of table elements\n",
        "        \"\"\"\n",
        "        for table_idx, table in enumerate(table_elements): #TODO: Find better way to index different tables on the same page? not all have class attributes we can ID them with.\n",
        "            try:\n",
        "                tags = table.find_all('sup')\n",
        "                for tag in tags:\n",
        "                    tag.extract()\n",
        "\n",
        "                # Attempt to find the title of this table. tableheader elements only tell us (pandas) how to index it, but what we need the header element for context on what this table is about\n",
        "                tablename = f\"table_{table_idx}\" # default name presuming none is found\n",
        "                for headertype in ['h3', 'h4']: # unlikely to lie in h2 or h1? could result in duplicate data. if we find by those.\n",
        "                    header = table.find_previous(headertype)\n",
        "                    if header is not None and header.text is not None: # find the closest header\n",
        "                        tablename = header.text.replace(\"\\n\", \"\").replace(\"#\", '').replace(\"\\r\", '').replace(\"/\", '')\n",
        "                        break\n",
        "                \n",
        "                tablename = os.path.basename(url) + \" \" + f\"{tablename}\" # what we will call this table, some has really annoying spacing, so maybe .replace(' ', '')?\n",
        "                print(f\"Grabbing table data under tablename {tablename}\")\n",
        "                df = pd.read_html(StringIO(str(table)), header=0)[0] # some tables do not have tableheader <th> tags for first row \n",
        "                # which would result in  generic column indices being created, so forcibly set first row as tableheader. \n",
        "                df['context'] = [tablename] * df.shape[0]\n",
        "\n",
        "                if len(tablename) >= os.pathconf('/', 'PC_NAME_MAX'): # prevent shit from exploding because my tablename is damn scuffed but what todo.\n",
        "                    tablename = tablename[:os.pathconf('/', 'PC_NAME_MAX') - 10]\n",
        "                csv_path = os.path.join(self.csv_path, tablename + '.csv')\n",
        "                # print(df)\n",
        "\n",
        "                # print(\"csv_path:\", csv_path)\n",
        "                # print(\"csv_path:\", csv_path)\n",
        "                df.to_csv( csv_path, index=False ) # if true will fuck up columning in csvloader\n",
        "                # loader = UnstructuredTSVLoader(csv_path, mode='elements')\n",
        "                loader = CSVLoader(file_path=csv_path,\n",
        "                                    csv_args={  'delimiter': ',',\n",
        "                                                'quotechar': '\"',\n",
        "                                                'fieldnames': [str(col) for col in df.columns]}\n",
        "                                    )\n",
        "                for row in loader.load()[1:]: # first row is just column indexes, so void\n",
        "                    self.tabular.append(\n",
        "                        self.text_splitter.split_text(\n",
        "                            self.clean_text(row.page_content)\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "            except BaseException as e:\n",
        "                print(f\"Unable to extract table data from url {url} with error {e}, passing!\")\n",
        "\n",
        "\n",
        "    def create_loaders(self):\n",
        "        \"\"\"\n",
        "        Seperate method to create the loaders. Directly appends to textual_data attribute and calls extract_table_elements to handle tabular data.\n",
        "        \"\"\"\n",
        "        for url, soupy_little_guy in self.raws:\n",
        "            loader = WebBaseLoader(\n",
        "                    web_paths=(url,),  # No URL fetching as we already have the HTML content\n",
        "                    bs_kwargs={\"parse_only\": SoupStrainer(['main'])},\n",
        "                )\n",
        "            html_content = loader.load()\n",
        "            for i in range(len(html_content)):\n",
        "                self.textual_data.extend(\n",
        "                    self.text_splitter.split_text(\n",
        "                        self.clean_text(\n",
        "                            html_content[i].page_content\n",
        "                )))\n",
        "\n",
        "            table_elements = soupy_little_guy.find_all(\"table\") \n",
        "            self.extract_table_elements(url=url, table_elements=table_elements)\n",
        "\n",
        "\n",
        "    def scrape_website(self, base_url, max_depth, depth=0):\n",
        "        \"\"\"\n",
        "        Wraps around a nested function get_from_website, so when ran will define a new function that knows that the sauce base_url is.\n",
        "        Scuffed? Yes.\n",
        "        \"\"\"\n",
        "\n",
        "        def get_from_website(url, max_depth, depth):\n",
        "            \"\"\"\n",
        "            Recursively scrape a website by visiting links starting from url. \n",
        "            Because this is a mostly I/O bound operation, we make a seperate method that actually creates the Loaders.\n",
        "\n",
        "            Parameters:\n",
        "                - url:              URL to start scraping from\n",
        "                - depth:            Current recursion depth\n",
        "                - max_depth:        Maximum recursion depth to avoid infinite loops\n",
        "\n",
        "            Returns:\n",
        "            - appends to self.raws, [url, BeautifulSoup object] created from response content.\n",
        "            \"\"\"\n",
        "            if url in self.visited_urls or depth > max_depth:\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                soupy_little_guy = BeautifulSoup(response.content, 'html.parser')\n",
        "                if response.status_code != 200:\n",
        "                    return print(f\"Failed to retrieve {url}\")\n",
        "            except Exception as e:\n",
        "                return print(f\"Error accessing {url} with error: {e}\")\n",
        "                \n",
        "\n",
        "            self.visited_urls.add(url)\n",
        "            print(\"Current url:\", url)\n",
        "            self.raws.append([url, soupy_little_guy])\n",
        "\n",
        "            for link in soupy_little_guy.find_all('a', href=True):  # Find all links on the current page\n",
        "                relative_url = link['href']\n",
        "                absolute_url = urllib.parse.urljoin(url, relative_url)\n",
        "                if base_url in absolute_url:  # Avoids external sites\n",
        "                    get_from_website(absolute_url, max_depth, depth + 1)\n",
        "\n",
        "        get_from_website(base_url, max_depth, depth)\n",
        "\n",
        "\n",
        "    def scrape_pdf(self, pdf_path):\n",
        "        \"\"\"\n",
        "        Extracts information from pdf files.\n",
        "        Texts stay as texts.\n",
        "        Tables and images...\n",
        "\n",
        "        Parameters:\n",
        "            - pdf_path:     PDF file to extract from\n",
        "\n",
        "        Output:\n",
        "            - textual_data: List of cleaned strings that represent data from pdf doc\n",
        "        \"\"\"\n",
        "        try:\n",
        "            pdf_document = fitz.open(pdf_path)\n",
        "        except:\n",
        "            return print(f\"Unable to open {pdf_path}\")\n",
        "        print(f\"Current pdf: {pdf_path}\")\n",
        "        pdf_text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document.load_page(page_num)\n",
        "            pdf_text += page.get_text(\"text\")\n",
        "\n",
        "        self.textual_data.extend(\n",
        "            self.text_splitter.split_text(\n",
        "                self.clean_text(pdf_text)\n",
        "        ))\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Cleans text retrieved from sources to reduce the storage needed\n",
        "\n",
        "        get_from_website(base_url, max_depth, depth)\n",
        "        Parameters:\n",
        "            - text:         Original string\n",
        "\n",
        "        Output:\n",
        "            - cleaned_text: Cleaned string \n",
        "        \"\"\"\n",
        "        return text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ')\n",
        "    \n",
        "    def embed_text(self, text):\n",
        "        \"\"\"\n",
        "        Embeds a given text if necessary\n",
        "\n",
        "        Parameters:\n",
        "            - text:     Original string\n",
        "\n",
        "        Output:\n",
        "            - text:     Embedded string \n",
        "        \"\"\"\n",
        "        return embedder.embed_query(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec1d94c",
      "metadata": {},
      "source": [
        "We define our websites and pdf links below to call the dataloader on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e340c158",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current url: https://www.iras.gov.sg/taxes\n",
            "Current pdf: ./data/pdfs\\verification-checklist-inspection-of-machines.pdf\n"
          ]
        }
      ],
      "source": [
        "# Max depth determines whether you wish to look into children of parents websites, else set to 0\n",
        "max_depth = 2\n",
        "websites = [\n",
        "    \"https://www.iras.gov.sg/taxes\",\n",
        "    \"https://www.iras.gov.sg/schemes\",\n",
        "    \"https://www.mom.gov.sg/passes-and-permits\",\n",
        "    \"https://www.mom.gov.sg/employment-practices\",\n",
        "    \"https://www.mom.gov.sg/workplace-safety-and-health\"\n",
        "]\n",
        "\n",
        "datahandler = DataHandler()\n",
        "for website in websites:\n",
        "    datahandler.scrape_website(website, max_depth)\n",
        "\n",
        "if os.path.isdir('./data/pdfs'):\n",
        "    for pdf in os.listdir('./data/pdfs'):\n",
        "        datahandler.scrape_pdf(os.path.join('./data/pdfs', pdf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "72414862",
      "metadata": {},
      "outputs": [],
      "source": [
        "datahandler.create_loaders() # only when we actually want to create loaders to debug stuff, so we don't need to scrape within the same function that we're (likely) debugging, saving time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c9a8d2",
      "metadata": {},
      "source": [
        "And then, we embed the textual and tabular data gathered for use in the vector db."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5b763dfb",
      "metadata": {},
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "[402] Payment Required\nAccount 'GIejbs1uP2wNdJHs1XWh8uHtgNwzs3BDRY4GFMZxNlw': Cloud credits expired - Please contact NVIDIA representatives",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m datahandler\u001b[38;5;241m.\u001b[39membedded_data \u001b[38;5;241m=\u001b[39m [\u001b[43mdatahandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m datahandler\u001b[38;5;241m.\u001b[39mtextual_data] \u001b[38;5;241m+\u001b[39m [datahandler\u001b[38;5;241m.\u001b[39membed_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m datahandler\u001b[38;5;241m.\u001b[39mtabular_data]\n\u001b[0;32m      2\u001b[0m datahandler\u001b[38;5;241m.\u001b[39mtextual_data\u001b[38;5;241m.\u001b[39mextend(datahandler\u001b[38;5;241m.\u001b[39mtabular_data)\n",
            "Cell \u001b[1;32mIn[42], line 230\u001b[0m, in \u001b[0;36mDataHandler.embed_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    Embeds a given text if necessary\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        - text:     Embedded string \u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\embeddings.py:156\u001b[0m, in \u001b[0;36mNVIDIAEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Input pathway for query embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\embeddings.py:142\u001b[0m, in \u001b[0;36mNVIDIAEmbeddings._embed\u001b[1;34m(self, texts, model_type)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate:\n\u001b[0;32m    140\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate\n\u001b[1;32m--> 142\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_req\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    146\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:369\u001b[0m, in \u001b[0;36mNVEModel.get_req\u001b[1;34m(self, payload, invoke_url)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m--> 369\u001b[0m response, session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(response, session)\n",
            "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:214\u001b[0m, in \u001b[0;36mNVEModel._post\u001b[1;34m(self, invoke_url, payload)\u001b[0m\n\u001b[0;32m    210\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_fn()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_authorization(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_inputs))\n\u001b[0;32m    213\u001b[0m )\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
            "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:311\u001b[0m, in \u001b[0;36mNVEModel._try_raise\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    309\u001b[0m     body \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease check or regenerate your API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mException\u001b[0m: [402] Payment Required\nAccount 'GIejbs1uP2wNdJHs1XWh8uHtgNwzs3BDRY4GFMZxNlw': Cloud credits expired - Please contact NVIDIA representatives"
          ]
        }
      ],
      "source": [
        "datahandler.embedded_data = [datahandler.embed_text(text) for text in datahandler.textual_data] + [datahandler.embed_text(text) for text in datahandler.tabular_data]\n",
        "datahandler.textual_data.extend(datahandler.tabular_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851b16b3-43ac-4269-9f37-05a33efe24fb",
      "metadata": {
        "id": "851b16b3-43ac-4269-9f37-05a33efe24fb"
      },
      "source": [
        "### 4) Storing the documents\n",
        "\n",
        "To build our foundational knowledge base from our collected data and allow for faster retrieval of vector queries, we need to have some form of search system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687a9681",
      "metadata": {},
      "source": [
        "#### a) Process the documents into vectorstore and save it to disk\n",
        "\n",
        "Vectorstores are good when we wish to store small datasets mainly in memory, thus using less storage.\n",
        "\n",
        "In this case, we use FAISS, which is a high-performace library that is efficient for similarity search and clustering of dense vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "804c85f6-181b-4291-a685-d6b378015544",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "804c85f6-181b-4291-a685-d6b378015544",
        "outputId": "ec8742d2-d131-4dfc-ab1e-854694194e0e"
      },
      "outputs": [],
      "source": [
        "# Here we create a faiss vector store from the documents and save it to disk.\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# You will only need to do this once, later on we will restore the already saved vectorstore\n",
        "store = FAISS.from_texts(datahandler.embedded_data, embedder)\n",
        "VECTOR_STORE = './data/nv_embedding'\n",
        "store.save_local(VECTOR_STORE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f867df18-11c8-45ea-b81c-1603459431f9",
      "metadata": {
        "id": "f867df18-11c8-45ea-b81c-1603459431f9"
      },
      "source": [
        "To enable runtime search, we index text chunks by embedding each document split and storing these embeddings in a vector database. Later to search, we embed the query and perform a similarity search to find the stored splits with embeddings most similar to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25619cae",
      "metadata": {},
      "source": [
        "Then, we can read the previously processed and saved vector store back for use;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413",
      "metadata": {
        "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Load the FAISS vectorestore back.\n",
        "VECTOR_STORE = './data/nv_embedding'\n",
        "store = FAISS.load_local(VECTOR_STORE, embedder, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0dbd352",
      "metadata": {},
      "source": [
        "#### b) Store all our data into a vectordb (connects vector stores to structured db systems)\n",
        "\n",
        "We will be using **Milvus DB** since it is an open-source, distributed vector DB designed for high-performance vector similarity search across massive datasets (which is what we kind of will have), and makes use of popular libraries like FAISS or Annoy for its vector searching.\n",
        "\n",
        "We have the option to use;\n",
        "1) MilvusVectorStore (Milvus Lite) which is easier to implement and can be more easily tied with ML frameworks.\n",
        "2) MilvusClient (Python SDK) WITHOUT Docker which was used in a demo to build a RAG system. --> What we shall try with first\n",
        "3) Direct DB WITH Docker (Milvus Standalone) to directly manage and control the connections as well as increase flexibility.\n",
        "\n",
        "Sources used:\n",
        "- https://milvus.io/docs/quickstart.md\n",
        "- https://milvus.io/docs/integrate_with_langchain.md\n",
        "- https://milvus.io/docs/multimodal_rag_with_milvus.md\n",
        "- https://github.com/milvus-io/bootcamp/tree/master/bootcamp/tutorials/quickstart/apps/multimodal_rag_with_milvus (Possible demo to use)\n",
        "\n",
        "##### Note:\n",
        "\n",
        "Previously, I thought of using **Weaviate** because it is a free, open-source, scalable and reliable vector database service with decent amounts of documentation online.\n",
        "\n",
        "The Weaviate DB works more with Object-Oriented design rather than rows/columns and does not need to be structured like an SQL DB since it already has integrated models/pre-computed embeddings to handle vector embeddings.\n",
        "\n",
        "However, I realised it **could not** be GPU accelerated during its vector retrieval/indexing.\n",
        "\n",
        "Sources used:\n",
        "- https://python.langchain.com/docs/integrations/vectorstores/weaviate/\n",
        "- https://weaviate.io/developers/weaviate/client-libraries/python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "52344def",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langchain-milvus\n",
        "!pip install pymilvus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e5c2dfbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "from langchain_milvus import Milvus, Zilliz\n",
        "\n",
        "class MilvusDB:\n",
        "    \"\"\"\n",
        "    Not-so-masterfully handles the vector database using Milvus and all DB related functionality.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = None\n",
        "        self.collection_name = None\n",
        "\n",
        "    def set_up_db(self):\n",
        "        \"\"\"\n",
        "        Starts up connection to Weaviate DB and creates schema if not already created\n",
        "\n",
        "        Parameters: None\n",
        "\n",
        "        Output:     None\n",
        "        \"\"\"\n",
        "        self.client = MilvusClient(uri=\"./milvus.db\")\n",
        "\n",
        "    def create_collection(self, collection_name, dimensions):\n",
        "        \"\"\"\n",
        "        Creates a new collection in the DB\n",
        "\n",
        "        Parameters: \n",
        "            - collection_name:  Name of collection to make\n",
        "            - dimensions:       Number of dimensions for vector data\n",
        "\n",
        "        Output:     None\n",
        "        \"\"\"     \n",
        "        if self.client.has_collection(collection_name):\n",
        "            self.client.drop_collection(collection_name)\n",
        "\n",
        "        self.client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            dimension=dimensions,\n",
        "            metric_type=\"IP\",  # Inner product distance\n",
        "            consistency_level=\"Strong\",  # Strong consistency level\n",
        "        )\n",
        "        self.collection_name = collection_name\n",
        "\n",
        "    def insert_data(self, original, embedded):\n",
        "        \"\"\"\n",
        "        Adds document and embedding object pairs to the DB collection \n",
        "\n",
        "        Parameters:\n",
        "            - original:     Original documents\n",
        "            - embedded:     Embedded documents\n",
        "\n",
        "        Output: None\n",
        "        \"\"\"\n",
        "        data = []\n",
        "\n",
        "        for i, embedded_line in enumerate(tqdm(embedded, desc=\"Creating embeddings\")):\n",
        "            data.append({\"id\": i, \"vector\": embedded_line, \"text\": original[i]})\n",
        "\n",
        "        self.client.insert(collection_name=self.collection_name, data=data)\n",
        "\n",
        "    def retrieve_data(self, question):\n",
        "        \"\"\"\n",
        "        Retrieves vector data from DB based on embedded question\n",
        "\n",
        "        Parameters:\n",
        "            - question:     Embedded question as a vector\n",
        "\n",
        "        Output:\n",
        "            - search_res:   Results of vector retrieval\n",
        "        \"\"\"\n",
        "        search_res = self.client.search(\n",
        "            collection_name=self.collection_name,\n",
        "            data=[question],  \n",
        "            limit=3,  # Return top 3 results\n",
        "            search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
        "            output_fields=[\"text\"],  # Return the text field\n",
        "        )\n",
        "        return search_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "89671eee",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'DataHandler' object has no attribute 'embedded_data'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m database \u001b[38;5;241m=\u001b[39m MilvusDB()\n\u001b[1;32m----> 2\u001b[0m database\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdatahandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_data\u001b[49m))\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'DataHandler' object has no attribute 'embedded_data'"
          ]
        }
      ],
      "source": [
        "database = MilvusDB()\n",
        "database.create_collection(\"Documents\", len(datahandler.embedded_data))\n",
        "database.insert_data(datahandler.embedded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d",
      "metadata": {
        "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d"
      },
      "source": [
        "### 5) Using data to answer questions\n",
        "\n",
        "With our stored embedded data, we can retrieve relevant vectors stored in our vectorstore/db to answer embedded questions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9ce3ba",
      "metadata": {},
      "source": [
        "#### a) Using vectorstore to answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d",
      "metadata": {
        "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "retriever = store.as_retriever()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer solely based on the following:\\n<Documents>\\n{context}\\n</Documents>\",\n",
        "        ),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Langchain's LCEL(LangChain Expression Language) Runnable protocol is used to define the chain\n",
        "# LCEL allows pipe together components and functions\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a0191d",
      "metadata": {},
      "source": [
        "#### b) Using vectordb to answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "198905d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(question):\n",
        "    question = embedder.embed_query(question)\n",
        "    retrieved_data = database.retrieve_data(question)\n",
        "    retrieved_texts = '\\n\\n'.join([hit.entity.get(\"text\") for hit in retrieved_data])\n",
        "    \n",
        "    prompt = f\" \\\n",
        "        Context from database:\\n{retrieved_texts}\\n\\n\\\n",
        "        User's question: {question}\\n\\n\\\n",
        "        Please generate an informative response.\\\n",
        "    \"\n",
        "    return llm.chat(prompt)\n",
        "\n",
        "# TODO: test if this works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d318b0",
      "metadata": {
        "id": "18d318b0"
      },
      "source": [
        "Case 1: Irrelevant question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9798ab63",
      "metadata": {
        "id": "9798ab63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is no information provided in the documents about a component or entity called \"two hundred and twenty five million.\" The documents contain information about various financial transactions, mortgage and loan balances, estate duty calculations, and rules related to M&A allowance and motor vehicle expenses, but there is no mention of a \"component\" with a value of 225 million.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"What is component two hundred and twenty five million?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f474d0ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(generate_response(\"What is component two hundred and twenty five million?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3223cefb",
      "metadata": {
        "id": "3223cefb"
      },
      "source": [
        "Case 2: Simple questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ba4ef8",
      "metadata": {
        "id": "28ba4ef8"
      },
      "outputs": [],
      "source": [
        "print(chain.invoke(\"How do i file taxes for my company?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b0f962",
      "metadata": {
        "id": "87b0f962"
      },
      "source": [
        "Case 3: Complex questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3c2e9c",
      "metadata": {
        "id": "af3c2e9c"
      },
      "outputs": [],
      "source": [
        "print(chain.invoke(\"In the event my foriegn employee is injured at work, how do i report the incident and claim reparations?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yxJMeKE-LItu",
      "metadata": {
        "id": "yxJMeKE-LItu"
      },
      "source": [
        "Case 4: Realistic questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "SM9lTRNlLItw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM9lTRNlLItw",
        "outputId": "16202adf-df26-453a-f34e-820968f35832"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'From 2025 onwards, as an employer whose firm qualifies for PWCS (Productivity Works Credits Scheme), you can receive co-funding from the government of up to 50% for the first tier of wage increases and 15% to 30% for the second tier of wage increases. This co-funding support applies to wage increases given in qualifying year 2025 and onwards. The gross monthly wage ceiling for PWCS co-funding will be increased to $3,000 in qualifying years 2025 and 2026.\\n\\nPlease note that the specific rates and details of the co-funding support may be subject to changes or updates in the PWCS guidelines. It is recommended to consult the official guidelines or contact the relevant authorities for the most accurate and up-to-date information.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"I am an employer whose firm qualifies for PWCs. From 2025 onwards, how much Co-Funding can I recieve from the government?\")\n",
        "# In its current state, the model does an ethan for this quite hard question, although it at least does retrieve some (few?) relevant PWCS info."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f",
      "metadata": {
        "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f"
      },
      "source": [
        "### 6) Enhancing accuracy for single data sources\n",
        "\n",
        "This example demonstrates how a re-ranking model can be used to combine retrieval results and improve accuracy during retrieval of documents.\n",
        "\n",
        "Typically, reranking is a critical piece of high-accuracy, efficient retrieval pipelines. Generally, there are two important use cases:\n",
        "\n",
        "- Combining results from multiple data sources\n",
        "- Enhancing accuracy for single data sources\n",
        "\n",
        "Here, we focus on demonstrating only the second use case. If you want to know more, check [here](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/docs/retrievers/nvidia_rerank.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682",
      "metadata": {
        "collapsed": true,
        "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# We will narrow the collection to 100 results and further narrow it to 10 with the reranker.\n",
        "retriever = store.as_retriever(search_kwargs={'k':100}) # typically k will be 1000 for real world use-cases\n",
        "ranker = NVIDIARerank(model='nv-rerank-qa-mistral-4b:1', top_n=10)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
        "        ),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "reranker = lambda input: ranker.compress_documents(query=input['question'], documents=input['context'])\n",
        "\n",
        "chain_with_ranker = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | {\"context\": reranker, \"question\": lambda input: input['question']}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1940aae",
      "metadata": {
        "id": "f1940aae",
        "outputId": "33a3f36f-5d7a-41a4-b919-56e724ff9f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided documents, to file taxes for your company, you need to follow these steps:\n",
            "\n",
            "1. Ensure that you are duly authorized by your company as an 'Approver' for Corporate Tax (Filing and Applications) in Corppass. You can refer to the step-by-step guides for assistance on Corppass setup.\n",
            "2. Have your Singpass and your companyâ€™s Unique Entity Number (UEN)/ Entity ID ready.\n",
            "3. Visit the mytax.iras.gov.sg website to file the Corporate Income Tax Return for your company.\n",
            "4. If your company is filing Form C, you need to submit the financial statements/certified accounts and tax computation(s) for the relevant Year of Assessment (YA).\n",
            "5. If your company meets the qualifying conditions to file Form C-S or Form C-S (Lite), you can choose to file the simplified version, Form C-S (Lite), if your company has an annual revenue of $200,000 or below.\n",
            "\n",
            "You can also visit the Basic Guide to Corporate Income Tax for Companies page to get help with filing your companyâ€™s tax returns for the first time. Additionally, you can access the New Company Start-Up Kit, an interactive guide, to learn more.\n"
          ]
        }
      ],
      "source": [
        "print(chain_with_ranker.invoke(\"How do i file taxes for my company?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30083cbb",
      "metadata": {
        "id": "30083cbb",
        "outputId": "a517c0c1-21cf-4b66-bf32-0cc299513ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided documents, if your foreign employee is injured at work, you can report the incident and claim reparations under the Work Injury Compensation Act (WICA). Specifically, the documents mention that input tax can be claimed for work injury compensation insurance that is obligatory under WICA for both local and foreign employees performing manual work or non-manual work earning $2,600 or less a month.\n",
            "\n",
            "To report the incident and make a claim, you can visit the Ministry of Manpower (MOM) webpage on WICA or contact MOM at +65 6438 5122. However, it is important to note that medical and accident insurance premiums for your staff are generally not allowable for input tax claims under the GST (General) Regulations, unless the insurance or payment of compensation is obligatory under WICA or under any collective agreement within the meaning of the Industrial Relations Act.\n",
            "\n",
            "Therefore, it seems that you can report the incident and claim reparations under WICA, but you should verify the specific requirements and conditions for your situation with MOM or your trade union.\n"
          ]
        }
      ],
      "source": [
        "print(chain_with_ranker.invoke(\"In the event my foriegn employee is injured at work, how do i report the incident and claim reparations?\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
